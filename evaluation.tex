\section{Evaluation and comparison with other FASTQ compression tools}\label
{sec:evaluation-and-comparison-with-other-fastq-compression-tools}

\subsection{Models}\label{subsec:models}

All the predefined models used in the tests below have been created using
publicly-available sequences processed by \emph{idencomp}.
The process for generating each model has followed the procedure below:

\begin{enumerate}
    \item Download the sequence and convert it to a FASTQ file.
    \item Use the \texttt{generate-models-all} \emph{idencomp} command to
    create the models for all possible context specifier types.
    \item By analyzing the statistics generated by \emph{idencomp}, choose
    the model which achieves the best compression rate and is not too big to
    be further processed.
    Choose one model for acids and one for quality scores.
    \item Use the \texttt{bin-contexts-all} \emph{idencomp} command to
    generate binned versions of the chosen models.
    \item By analyzing the statistics, choose the model significantly smaller
    than the original but still achieving a decent compression rate.
\end{enumerate}

\begin{figure}[h]
    \centering
    \input{img/binning}
    \caption{Relationship between number of contexts and compression rate
    penalty for all binned versions of the quality scores model generated
    from the sample SRR16141966}
    \label{fig:binning-example}
\end{figure}

\Cref{fig:binning-example} shows the relationship between the number of
contexts and the compression rate.
The compression rate here is expressed as the penalty relative to the
original, unbinned model.
Notice that the bits-per-value differences between the binned versions are
most significant at the low number of bins (up to a few hundred contexts) and
get very small later;
this is true for virtually all tested models.
Usually, there are a few compression rate penalty spikes between binned
versions in the middle (as can be seen around context number 21,000 here).
The binned versions just after those spikes might be good candidates to be
used as the final, compact versions of the model.
In this specific case, the model with 463 contexts was kept --- as one can
see on the plot, there is a spike there as well.

All of the pre-defined model details can be seen in
\Cref{sec:pre-defined-models}.
Notice that some models achieve excellent compression rates even though they
are tens of times smaller than the originals, and some require to be kept
large to avoid harming the rate too much.

\subsection{Test details}\label{subsec:test-details}

The tests consider the compression ratio and compression/decompression speed
(and the relationship between those values).
We define the compression speed as the original data size divided by the
processing time.
The same applies to the decompression speed (resulting file size divided by
the decompression time).
The tests do not take model sizes into account --- however, they are
insignificant in most cases, as most of the tests are performed on 1GB files,
while all of the models take about 30MB of disk space.
Please consult \Cref{sec:benchmarking-tools-and-environment} for the exact
testing environment and the tools that are tested.

\newpage

\subsection{Small sample}\label{subsec:small-data}

The test with a small file (first 10MB of SRR19549058) shows that the
preprocessing of models takes a significant time and overwhelms any other
processing cost.
The compressor and decompressor load all models before processing the file,
which means there is an almost constant startup delay.
Therefore, while compression speed is on par with \emph{gzip}, decompression
is much slower.

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \centering
        \input{img/bench-small-data-compress}
        \caption{Compression}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \vspace{1em}
        \input{img/bench-small-data-decompress}
        \caption{Decompression}
    \end{subfigure}
    \caption{%
        Compressor benchmark for the first 10MB of SRR19549058
    }
    \label{fig:bench-small-data}
\end{figure}

\newpage

\subsection{Sample with a dedicated model}\label{subsec:sample-with-a
-dedicated-model}

This test has been performed using the first 1GB of sample SRR2962693, which
has a dedicated model in \emph{idencomp}.
Because the input data is much larger than in the previous test, the startup
time is not significant anymore.

\emph{idencomp} performs similarly to the other general compressors (like
\emph{gzip} and \emph{bzip2}), but worse than dedicated sequence compressing
tools, both in terms of performance and compression ratio.
It also seems that \texttt{{-}{-}quality=1} option (which reduces the number
of models used during compression) works pretty well here.
There is rarely more than one model needed to compress a sequence, so the
mentioned option improves the performance almost without harming the
compression ratio.

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \centering
        \input{img/bench-dedicated-model-compress}
        \caption{Compression}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \vspace{1em}
        \input{img/bench-dedicated-model-decompress}
        \caption{Decompression}
    \end{subfigure}
    \caption{%
        Compressor benchmark for the first 1GB of SRR2962693
    }
    \label{fig:bench-dedicated-model}
\end{figure}

\newpage

\subsection{Unknown sample with a known sequencing method}
\label{subsec:sample-with-a -known-sequencing-method}

This test has been performed using the first 1GB of sample SRR2747516.
This sample does not have a dedicated model, but is similar to other known
samples --- it's a sequencing of a mammal (Shiba Inu dog) that has been
performed using Illumina HiSeq 2500.

\emph{idencomp} performs slightly worse here in the terms of the compression
ratio.
It's worse than general compression tools as well.
The performance is comparable both with general-purpose and dedicated
compression tools, however.

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \centering
        \input{img/bench-known-sequencing-compress}
        \caption{Compression}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \vspace{1em}
        \input{img/bench-known-sequencing-decompress}
        \caption{Decompression}
    \end{subfigure}
    \caption{%
        Compressor benchmark for the first 1GB of SRR2747516
    }
    \label{fig:bench-known-sequencing}
\end{figure}

\newpage

\subsection{Best performance}\label{subsec:best-performance}

This test aims to improve the performance.
It has been performed using the first 1GB of sample SRR5373739.
This sample has a dedicated tiny model (eight acid contexts and six quality
score contexts), so that it can easily fit into the CPU cache.
\emph{idencomp} has been forced to use this specific model by removing all
other models, so the startup time is also lower.
In addition to that, the block size has been increased, which seems to
improve performance with large files.

While \emph{idencomp} does not achieve the best compression ratio, the
performance is excellent.
It outperforms almost all other tools in compression speed, and maintains a
decent decompression speed as well.

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \centering
        \input{img/bench-best-performance-compress.tex}
        \caption{Compression}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \vspace{1em}
        \input{img/bench-best-performance-decompress.tex}
        \caption{Decompression}
    \end{subfigure}
    \caption{%
        Compressor benchmark for the first 1GB of SRR5373739.
    }
    \label{fig:bench-best-performance}
\end{figure}

\newpage

\subsection{Better compression ratio}\label{subsec:better-ratio}

The last test tries to improve the compression ratio without hurting the
performance built in the previous test.
As before, \emph{idencomp} uses a tiny model built specifically for a file
--- this time, sample SRR2962693 is reused, but the model has been built
using only the first gigabyte of the file.
The second gigabyte is used as a test file to be compressed.

The main reason for the poor compression ratio of \emph{idencomp} is that
every single sequence takes about nine additional bytes for the headers to be
serialized, most of which is occupied by the length of a sequence, both
compressed and uncompressed.
In practice, all the sequence lengths in a single file are usually equal (as
in SRR2962693, where each sequence has a length of 126).
Therefore, this test uses an evaluation file modified to our advantage ---
the subsequent sequences are glued together until they are at least 10000
symbols long.
This modified file is then compressed.

\emph{idencomp} performs really well here.
Not only is it faster than almost all other compression tools, it also
maintains a compression ratio better than all general-purpose compressors.
Other dedicated compression methods achieve similar compression ratio.
See \Cref{fig:bench-better-ratio} for the details.

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \centering
        \input{img/bench-better-ratio-compress.tex}
        \caption{Compression}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \vspace{1em}
        \input{img/bench-better-ratio-decompress.tex}
        \caption{Decompression}
    \end{subfigure}
    \caption{%
        Compressor benchmark for the second gigabyte of SRR2962693.
    }
    \label{fig:bench-better-ratio}
\end{figure}
